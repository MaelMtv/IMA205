{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from skimage.morphology import disk\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.saving import load_model\n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all image paths from train set and create list of names for their destination after preprocessing\n",
    "\n",
    "train_ims = sorted(glob(os.path.join('.\\Train\\Train','*.jpg')))\n",
    "print(len(train_ims))\n",
    "print(train_ims[:5])\n",
    "filenametrain = []\n",
    "for i in range(len(train_ims)):\n",
    "    name = train_ims[i]\n",
    "    name = name.replace('.\\\\Train\\\\Train\\\\','./Train/Modified/')\n",
    "    filenametrain.append(name)\n",
    "print(filenametrain[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for the test set\n",
    "\n",
    "test_ims = sorted(glob(os.path.join('.\\Test\\Test','*.jpg')))\n",
    "print(len(test_ims))\n",
    "print(test_ims[:5])\n",
    "filenametest = []\n",
    "for i in range(len(test_ims)):\n",
    "    name = test_ims[i]\n",
    "    name = name.replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "    filenametest.append(name)\n",
    "print(filenametest[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define line kernel for the tophat and clahe operator for local histogram equalization to enhance image contrast\n",
    "kernel = cv.getStructuringElement(1,(9,9))\n",
    "clahe = cv.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply dull razor algorithm for hair removal then clahe on each image\n",
    "for i in range(len(train_ims)):\n",
    "    if i%100 ==0:\n",
    "        print(i)\n",
    "    im = cv.resize(cv.imread(train_ims[i],cv.IMREAD_GRAYSCALE),(512,384))\n",
    "    blackhat = cv.morphologyEx(im, cv.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "    bhg= cv.GaussianBlur(blackhat,(3,3),cv.BORDER_DEFAULT)\n",
    "    _,mask = cv.threshold(bhg,1,255,cv.THRESH_BINARY)\n",
    "\n",
    "    dst = cv.inpaint(im,mask,6,cv.INPAINT_TELEA)\n",
    "\n",
    "    mask = cv.dilate((dst<15).astype(np.uint8),disk(30))\n",
    "    post = cv.inpaint(dst,mask,6,cv.INPAINT_TELEA)\n",
    "\n",
    "    fim = clahe.apply(post)\n",
    "    cv.imwrite(filenametrain[i],fim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_ims)):\n",
    "    if i%100 ==0:\n",
    "        print(i)\n",
    "    im = cv.resize(cv.imread(test_ims[i],cv.IMREAD_GRAYSCALE),(512,384))\n",
    "    blackhat = cv.morphologyEx(im, cv.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "    bhg= cv.GaussianBlur(blackhat,(3,3),cv.BORDER_DEFAULT)\n",
    "    _,mask = cv.threshold(bhg,1,255,cv.THRESH_BINARY)\n",
    "\n",
    "    dst = cv.inpaint(im,mask,6,cv.INPAINT_TELEA)\n",
    "\n",
    "    mask = cv.dilate((dst<15).astype(np.uint8),disk(30))\n",
    "    post = cv.inpaint(dst,mask,6,cv.INPAINT_TELEA)\n",
    "\n",
    "    fim = clahe.apply(post)\n",
    "    cv.imwrite(filenametest[i],fim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define segmentation CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get image paths for images that already have a segmentation as well as the path to this segmentation\n",
    "\n",
    "seg_train = sorted(glob(os.path.join('.\\Train\\Train','*.png')))\n",
    "print(len(seg_train))\n",
    "print(seg_train[:5])\n",
    "train_ims = []\n",
    "for i in range(len(seg_train)):\n",
    "    name = seg_train[i]\n",
    "    name = name.replace('_seg.png','.jpg')\n",
    "    name = name.replace('.\\\\Train\\\\Train\\\\','./Train/Modified/')\n",
    "    train_ims.append(name)\n",
    "print(train_ims[:5])\n",
    "\n",
    "\n",
    "\n",
    "seg_test = sorted(glob(os.path.join('.\\Test\\Test','*.png')))\n",
    "print(len(seg_test))\n",
    "print(seg_test[:5])\n",
    "test_ims = []\n",
    "for i in range(len(seg_test)):\n",
    "    name = seg_test[i]\n",
    "    name = name.replace('_seg.png','.jpg')\n",
    "    name = name.replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "    test_ims.append(name)\n",
    "print(test_ims[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define CNN\n",
    "\n",
    "in_layer = Input((384,512,1), name='input')\n",
    "conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(in_layer)\n",
    "conv2 = Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)(conv1)\n",
    "conv3 = Conv2D(4, (3, 3), activation='relu', padding='same', strides=2)(conv2)\n",
    "conv4 = Conv2DTranspose(4, kernel_size=3, strides=2, activation='relu', padding='same')(conv3)\n",
    "conv5 = Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same')(conv4)\n",
    "conv6 = Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same')(conv5)\n",
    "out_layer = Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')(conv6)\n",
    "\n",
    "autoencoder = keras.Model(in_layer,out_layer)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image for training\n",
    "autoencoder.compile(optimizer='adam', loss=MeanSquaredError())\n",
    "train_ims = np.array([cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384))/255 for x in train_ims])\n",
    "train_masks = np.array([cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384))/255 for x in seg_train])\n",
    "\n",
    "test_ims = np.array([cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384))/255 for x in test_ims])\n",
    "test_masks = np.array([cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384))/255 for x in seg_test])\n",
    "\n",
    "print(train_ims.shape)\n",
    "print(test_ims.shape)\n",
    "print(train_masks.shape)\n",
    "print(test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resahpe data\n",
    "data = train_ims.reshape((1945,384,512,1))\n",
    "masks = train_masks.reshape((1945,384,512,1))\n",
    "test_data = test_ims.reshape((648,384,512,1))\n",
    "test_masks = test_masks.reshape((648,384,512,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit model and save it\n",
    "h = autoencoder.fit(data,masks,epochs=10,batch_size=32,verbose=1,shuffle=True,validation_data=(test_data,test_masks))\n",
    "autoencoder.save('autoencoder_seg.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding best CNN output threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get images and masks for all test images that already have masks\n",
    "seg_test = sorted(glob(os.path.join('.\\Test\\Test','*.png')))\n",
    "print(len(seg_test))\n",
    "test_ims = []\n",
    "for i in range(len(seg_test)):\n",
    "    name = seg_test[i]\n",
    "    name = name.replace('_seg.png','.jpg')\n",
    "    name = name.replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "    test_ims.append(name)\n",
    "print(test_ims[:5])\n",
    "\n",
    "test_data = np.array([np.reshape(cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384)),(384,512,1)) for x in test_ims])\n",
    "test_masks = 1/255*np.array([np.reshape(cv.resize(cv.imread(y,cv.IMREAD_GRAYSCALE),(512,384)),(384,512,1)) for y in seg_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "autoencoder = load_model(\"autoencoder_seg.keras\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "s = np.zeros((648,255))\n",
    "m = np.concatenate((np.zeros((384,64)),np.concatenate((np.ones((384,384)),np.zeros((384,64))),axis=1)),axis=1)\n",
    "\n",
    "for i in range(0,648,2):\n",
    "    print(i)\n",
    "    z = test_data[i:i+2]/255\n",
    "    p = autoencoder(z).numpy()\n",
    "    for k in range(2):\n",
    "        mask = test_masks[i+k]\n",
    "        area = np.sum(mask)\n",
    "        \n",
    "        mi = np.min(p[k])\n",
    "        ma = np.max(p[k])\n",
    "        #Bring the image back between 0 and 255\n",
    "        ad = 255/(ma-mi)*(p[k]-mi)\n",
    "        \n",
    "        for t in range(255):\n",
    "            da = (ad>t).astype(np.uint8)\n",
    "\n",
    "            #Verify is there are big objects on the sides\n",
    "            s0 = np.sum(ad,axis=0)\n",
    "            s1 = np.sum(s0[:128])\n",
    "            s2 = np.sum(s0[128:256])\n",
    "            s3 = np.sum(s0[256:384])\n",
    "            if s1>s2 or s3>s2:\n",
    "                da = (da.reshape(384,512)*m).astype(np.uint8)\n",
    "            \n",
    "            #Keep only the biggest object in the image\n",
    "            da = cv.morphologyEx(da, cv.MORPH_CLOSE, cv.getStructuringElement(cv.MORPH_ELLIPSE, (60, 45)))\n",
    "            da = np.pad(da,((1,1),(1,1)),'constant',constant_values=(0,0))\n",
    "            ## Find largest contour in intermediate image\n",
    "            cnts, _ = cv.findContours(da, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "            if len(cnts)!=0:\n",
    "                cnt = max(cnts, key=cv.contourArea)\n",
    "                # Output\n",
    "                out = np.zeros(da.shape, np.uint8)\n",
    "                cv.drawContours(out, [cnt], -1, 1, cv.FILLED)\n",
    "\n",
    "                diff = np.abs(mask.reshape((384,512))-out[1:385,1:513])\n",
    "                area_ratio = np.sum(diff)/area\n",
    "                \n",
    "                s[i+k,t] = area_ratio\n",
    "\n",
    "err = np.sum(s,axis=0)\n",
    "best_t = np.argmin(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting images without masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = load_model(\"autoencoder_seg.keras\")\n",
    "ae.summary()\n",
    "threshold = best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only images who don't have a mask already\n",
    "train_ims = sorted(glob(os.path.join('.\\Train\\Train','*.jpg')))\n",
    "for i in range(len(train_ims)):\n",
    "    train_ims[i] = train_ims[i].replace('.\\\\Train\\\\Train\\\\','./Train/Modified/')\n",
    "print(len(train_ims))\n",
    "print(train_ims[:5])\n",
    "\n",
    "seg_ims = sorted(glob(os.path.join('.\\Train\\Train','*.png')))\n",
    "print(len(seg_ims))\n",
    "\n",
    "for i in range(len(seg_ims)):\n",
    "    seg_ims[i] = seg_ims[i].replace('_seg.png','.jpg')\n",
    "    seg_ims[i] = seg_ims[i].replace('.\\\\Train\\\\Train\\\\','./Train/Modified/')\n",
    "print(seg_ims[:5])\n",
    "\n",
    "undone_train = np.array([x for x in train_ims if x not in seg_ims])\n",
    "print(undone_train[:5])\n",
    "\n",
    "\n",
    "test_ims = sorted(glob(os.path.join('.\\Test\\Test','*.jpg')))\n",
    "for i in range(len(test_ims)):\n",
    "    test_ims[i] = test_ims[i].replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "print(len(test_ims))\n",
    "print(test_ims[:5])\n",
    "\n",
    "segtest_ims = sorted(glob(os.path.join('.\\Test\\Test','*.png')))\n",
    "print(len(segtest_ims))\n",
    "\n",
    "for i in range(len(segtest_ims)):\n",
    "    segtest_ims[i] = segtest_ims[i].replace('_seg.png','.jpg')\n",
    "    segtest_ims[i] = segtest_ims[i].replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "print(segtest_ims[:5])\n",
    "\n",
    "undone_test = np.array([x for x in test_ims if x not in segtest_ims])\n",
    "print(undone_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create names list for the future segmentation masks\n",
    "filenametrain = []\n",
    "for i in range(len(undone_train)):\n",
    "    name = undone_train[i]\n",
    "    name = name.replace('.\\\\Train\\\\Train\\\\','./Train/Modified/')\n",
    "    name = name.replace('.jpg','_seg.png')\n",
    "    filenametrain.append(name)\n",
    "print(filenametrain[:5])\n",
    "\n",
    "filenametest = []\n",
    "for i in range(len(undone_test)):\n",
    "    name = undone_test[i]\n",
    "    name = name.replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "    name = name.replace('.jpg','_seg.png')\n",
    "    filenametest.append(name)\n",
    "print(filenametest[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and save masks for the train set\n",
    "m = np.concatenate((np.zeros((384,64)),np.concatenate((np.ones((384,384)),np.zeros((384,64))),axis=1)),axis=1)\n",
    "for i in range(0,len(undone_train),2):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    z = np.array([1/255*np.reshape(cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384)),(384,512,1)) for x in undone_train[i:i+2]])\n",
    "    p = ae(z).numpy()\n",
    "    for k in range(2):\n",
    "        mi = np.min(p[k])\n",
    "        ma = np.max(p[k])\n",
    "        im = 255/(ma-mi)*(p[k]-mi)\n",
    "        thresh = (im>threshold).astype(np.uint8)\n",
    "\n",
    "        s0 = np.sum(thresh,axis=0)\n",
    "        s1 = np.sum(s0[:128])\n",
    "        s2 = np.sum(s0[128:256])\n",
    "        s3 = np.sum(s0[256:384])\n",
    "        if s1>s2 or s3>s2:\n",
    "            thresh = (thresh.reshape(384,512)*m).astype(np.uint8)\n",
    "        \n",
    "        thresh = cv.morphologyEx(thresh, cv.MORPH_CLOSE, cv.getStructuringElement(cv.MORPH_ELLIPSE, (60, 45)))\n",
    "        thresh = np.pad(255*thresh,((1,1),(1,1)),'constant',constant_values=(0,0))\n",
    "        # Find largest contour in intermediate image\n",
    "        cnts, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "        if len(cnts)!=0:\n",
    "            cnt = max(cnts, key=cv.contourArea)\n",
    "            # Output\n",
    "            out = np.zeros(thresh.shape, np.uint8)\n",
    "            cv.drawContours(out, [cnt], -1, 255,cv.FILLED)\n",
    "         \n",
    "            cv.imwrite(filenametrain[i+k],out[1:385,1:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for the test set\n",
    "for i in range(0,len(undone_test),2):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    z = np.array([1/255*np.reshape(cv.resize(cv.imread(x,cv.IMREAD_GRAYSCALE),(512,384)),(384,512,1)) for x in undone_test[i:i+2]])\n",
    "    p = ae(z).numpy()\n",
    "    for k in range(2):\n",
    "        mi = np.min(p[k])\n",
    "        ma = np.max(p[k])\n",
    "        im = 255/(ma-mi)*(p[k]-mi)\n",
    "        thresh = (im>threshold).astype(np.uint8)\n",
    "\n",
    "        s0 = np.sum(thresh,axis=0)\n",
    "        s1 = np.sum(s0[:128])\n",
    "        s2 = np.sum(s0[128:256])\n",
    "        s3 = np.sum(s0[256:384])\n",
    "        if s1>s2 or s3>s2:\n",
    "            thresh = (thresh.reshape(384,512)*m).astype(np.uint8)\n",
    "        \n",
    "        thresh = cv.morphologyEx(thresh, cv.MORPH_CLOSE, cv.getStructuringElement(cv.MORPH_ELLIPSE, (60, 45)))\n",
    "        thresh = np.pad(255*thresh,((1,1),(1,1)),'constant',constant_values=(0,0))\n",
    "        # Find largest contour in intermediate image\n",
    "        cnts, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "        if len(cnts)!=0:\n",
    "            cnt = max(cnts, key=cv.contourArea)\n",
    "            # Output\n",
    "            out = np.zeros(thresh.shape, np.uint8)\n",
    "            cv.drawContours(out, [cnt], -1, 255,cv.FILLED)\n",
    "         \n",
    "            cv.imwrite(filenametest[i+k],out[1:385,1:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add existing masks to the modified folders\n",
    "seg_ims = sorted(glob(os.path.join('.\\Train\\Train','*.png')))\n",
    "print(len(seg_ims))\n",
    "print(seg_ims[:5])\n",
    "train_dest = []\n",
    "for i in range(len(seg_ims)):\n",
    "    name = seg_ims[i]\n",
    "    name = name.replace('.\\\\Train\\\\Train\\\\','./Train/Modified/')\n",
    "    train_dest.append(name)\n",
    "print(train_dest[:5])\n",
    "\n",
    "for i in range(len(seg_ims)):\n",
    "    im = cv.resize(cv.imread(seg_ims[i],cv.IMREAD_GRAYSCALE),(512,384))\n",
    "    cv.imwrite(train_dest[i],im)\n",
    "\n",
    "\n",
    "\n",
    "segtest_ims = sorted(glob(os.path.join('.\\Test\\Test','*.png')))\n",
    "print(len(segtest_ims))\n",
    "print(segtest_ims[:5])\n",
    "test_dest = []\n",
    "for i in range(len(segtest_ims)):\n",
    "    name = segtest_ims[i]\n",
    "    name = name.replace('.\\\\Test\\\\Test\\\\','./Test/Modified/')\n",
    "    test_dest.append(name)\n",
    "print(test_dest[:5])\n",
    "\n",
    "for i in range(len(segtest_ims)):\n",
    "    im = cv.resize(cv.imread(segtest_ims[i],cv.IMREAD_GRAYSCALE),(512,384))\n",
    "    cv.imwrite(test_dest[i],im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set black masks for the few images that don't have any\n",
    "segtest_ims = sorted(glob(os.path.join('.\\Test\\Modified','*.png')))\n",
    "print(len(segtest_ims))\n",
    "print(segtest_ims[:5])\n",
    "test_ims = sorted(glob(os.path.join('.\\Test\\Modified','*.jpg')))\n",
    "print(len(test_ims))\n",
    "print(test_ims[:5])\n",
    "\n",
    "leftout_test=[]\n",
    "segtest_names = []\n",
    "for i in range(len(segtest_ims)):\n",
    "    name = segtest_ims[i]\n",
    "    name = name.replace('.\\\\Test\\\\Modified\\\\','')\n",
    "    name = name.replace('_seg.png','')\n",
    "    segtest_names.append(name)\n",
    "test_names = []\n",
    "for i in range(len(test_ims)):\n",
    "    name = test_ims[i]\n",
    "    name = name.replace('.\\\\Test\\\\Modified\\\\','')\n",
    "    name = name.replace('.jpg','')\n",
    "    test_names.append(name)\n",
    "print(segtest_names[:5])\n",
    "print(test_names[:5])\n",
    "leftout_test = [x for x in test_names if x not in segtest_names]\n",
    "print(len(leftout_test))\n",
    "print(leftout_test[:5])\n",
    "leftout_seg = []\n",
    "for i in range(len(leftout_test)):\n",
    "    name = leftout_test[i]\n",
    "    name = './Test/Modified/'+name+'_seg.png'\n",
    "    leftout_seg.append(name)\n",
    "print(leftout_seg[:5])\n",
    "for i in range(len(leftout_seg)):\n",
    "    im = np.zeros((384,512))\n",
    "    cv.imwrite(leftout_seg[i],im)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "segtrain_ims = sorted(glob(os.path.join('.\\Train\\Modified','*.png')))\n",
    "print(len(segtrain_ims))\n",
    "print(segtrain_ims[:5])\n",
    "train_ims = sorted(glob(os.path.join('.\\Train\\Modified','*.jpg')))\n",
    "print(len(train_ims))\n",
    "print(train_ims[:5])\n",
    "\n",
    "leftout_train=[]\n",
    "segtrain_names = []\n",
    "for i in range(len(segtrain_ims)):\n",
    "    name = segtrain_ims[i]\n",
    "    name = name.replace('.\\\\Train\\\\Modified\\\\','')\n",
    "    name = name.replace('_seg.png','')\n",
    "    segtrain_names.append(name)\n",
    "train_names = []\n",
    "for i in range(len(train_ims)):\n",
    "    name = train_ims[i]\n",
    "    name = name.replace('.\\\\Train\\\\Modified\\\\','')\n",
    "    name = name.replace('.jpg','')\n",
    "    train_names.append(name)\n",
    "print(segtrain_names[:5])\n",
    "print(train_names[:5])\n",
    "leftout_train = [x for x in train_names if x not in segtrain_names]\n",
    "print(len(leftout_train))\n",
    "print(leftout_train[:5])\n",
    "leftout_seg = []\n",
    "for i in range(len(leftout_train)):\n",
    "    name = leftout_train[i]\n",
    "    name = './Train/Modified/'+name+'_seg.png'\n",
    "    leftout_seg.append(name)\n",
    "print(leftout_seg[:5])\n",
    "for i in range(len(leftout_seg)):\n",
    "    im = np.zeros((384,512))\n",
    "    cv.imwrite(leftout_seg[i],im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Missing metadata values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./metadataTrain.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking care of missing sex values\n",
    "missing_sex = train_data[train_data['SEX'].isna()]\n",
    "print(missing_sex.count())\n",
    "print(missing_sex.head())\n",
    "\n",
    "sex = train_data[train_data['SEX'].isin(['male','female'])]\n",
    "print(sex.count())\n",
    "print(sex.head())\n",
    "\n",
    "fem = sex[sex['SEX']=='female']\n",
    "print(fem.count())\n",
    "print(fem.head())\n",
    "\n",
    "mal = sex[sex['SEX']=='male']\n",
    "print(mal.count())\n",
    "print(mal.head())\n",
    "\n",
    "male_prob = mal['ID'].count()/sex['ID'].count()\n",
    "print(male_prob)\n",
    "\n",
    "def f(x):\n",
    "    if x in ['male','female']:\n",
    "        return x\n",
    "    else:\n",
    "        p = np.random.random()\n",
    "        if p>male_prob:\n",
    "            return 'female'\n",
    "        else:\n",
    "            return 'male'\n",
    "    \n",
    "train_data['SEX'] = train_data['SEX'].map(f)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking care of missing age values\n",
    "age = train_data[train_data['AGE'].notna()]\n",
    "age.count()\n",
    "\n",
    "agenp = age['AGE'].to_numpy()\n",
    "print(agenp)\n",
    "min_age = np.min(agenp)\n",
    "max_age = np.max(agenp)\n",
    "print(min_age,max_age)\n",
    "\n",
    "def g(x):\n",
    "    if np.isnan(x):\n",
    "        return np.random.randint(min_age,max_age+1)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "train_data['AGE'] = train_data['AGE'].map(g)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking care of missing position\n",
    "print(train_data['POSITION'].dropna().count())\n",
    "poss = np.unique(train_data['POSITION'].dropna().to_numpy())\n",
    "print(poss)\n",
    "\n",
    "good_pos = train_data['POSITION'].dropna()\n",
    "\n",
    "ant = good_pos[good_pos==poss[0]].count()\n",
    "head = good_pos[good_pos==poss[1]].count()\n",
    "lat = good_pos[good_pos==poss[2]].count()\n",
    "low = good_pos[good_pos==poss[3]].count()\n",
    "ora = good_pos[good_pos==poss[4]].count()\n",
    "pal = good_pos[good_pos==poss[5]].count()\n",
    "pos = good_pos[good_pos==poss[6]].count()\n",
    "upp = good_pos[good_pos==poss[7]].count()\n",
    "tot = ant+head+lat+low+ora+pal+pos+upp\n",
    "print(ant,head,lat,low,ora,pal,pos,upp)\n",
    "print(tot)\n",
    "#Compute probabilities of each position\n",
    "cnts = [ant,head,lat,low,ora,pal,pos,upp]\n",
    "probs = cnts/tot\n",
    "ints = np.cumsum(probs)\n",
    "print(cnts)\n",
    "print(probs)\n",
    "print(ints)\n",
    "\n",
    "def h(x):\n",
    "    if x in poss:\n",
    "        return x\n",
    "    else:\n",
    "        p = np.random.random()\n",
    "        if p<ints[0]:\n",
    "            return poss[0]\n",
    "        elif p<ints[1]:\n",
    "            return poss[1]\n",
    "        elif p<ints[2]:\n",
    "            return poss[2]\n",
    "        elif p<ints[3]:\n",
    "            return poss[3]\n",
    "        elif p<ints[4]:\n",
    "            return poss[4]\n",
    "        elif p<ints[5]:\n",
    "            return poss[5]\n",
    "        elif p<ints[6]:\n",
    "            return poss[6]\n",
    "        else:\n",
    "            return poss[7]       \n",
    "    \n",
    "train_data['POSITION'] = train_data['POSITION'].map(h)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.notna().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('./metadataTrainFilled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do the same for test metadata\n",
    "test_data = pd.read_csv('./metadataTest.csv')\n",
    "missing_sex = test_data[test_data['SEX'].isna()]\n",
    "sex = test_data[test_data['SEX'].isin(['male','female'])]\n",
    "mal = sex[sex['SEX']=='male']\n",
    "male_prob = mal['ID'].count()/sex['ID'].count()\n",
    "\n",
    "def f(x):\n",
    "    if x in ['male','female']:\n",
    "        return x\n",
    "    else:\n",
    "        p = np.random.random()\n",
    "        if p>male_prob:\n",
    "            return 'female'\n",
    "        else:\n",
    "            return 'male'\n",
    "    \n",
    "test_data['SEX'] = test_data['SEX'].map(f)\n",
    "\n",
    "\n",
    "\n",
    "age = test_data[test_data['AGE'].notna()]\n",
    "agenp = age['AGE'].to_numpy()\n",
    "min_age = np.min(agenp)\n",
    "max_age = np.max(agenp)\n",
    "\n",
    "def g(x):\n",
    "    if np.isnan(x):\n",
    "        return np.random.randint(min_age,max_age+1)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "test_data['AGE'] = test_data['AGE'].map(g)\n",
    "\n",
    "\n",
    "\n",
    "poss = np.unique(test_data['POSITION'].dropna().to_numpy())\n",
    "good_pos = test_data['POSITION'].dropna()\n",
    "\n",
    "ant = good_pos[good_pos==poss[0]].count()\n",
    "head = good_pos[good_pos==poss[1]].count()\n",
    "lat = good_pos[good_pos==poss[2]].count()\n",
    "low = good_pos[good_pos==poss[3]].count()\n",
    "ora = good_pos[good_pos==poss[4]].count()\n",
    "pal = good_pos[good_pos==poss[5]].count()\n",
    "pos = good_pos[good_pos==poss[6]].count()\n",
    "upp = good_pos[good_pos==poss[7]].count()\n",
    "tot = ant+head+lat+low+ora+pal+pos+upp\n",
    "\n",
    "cnts = [ant,head,lat,low,ora,pal,pos,upp]\n",
    "probs = cnts/tot\n",
    "ints = np.cumsum(probs)\n",
    "\n",
    "def h(x):\n",
    "    if x in poss:\n",
    "        return x\n",
    "    else:\n",
    "        p = np.random.random()\n",
    "        if p<ints[0]:\n",
    "            return poss[0]\n",
    "        elif p<ints[1]:\n",
    "            return poss[1]\n",
    "        elif p<ints[2]:\n",
    "            return poss[2]\n",
    "        elif p<ints[3]:\n",
    "            return poss[3]\n",
    "        elif p<ints[4]:\n",
    "            return poss[4]\n",
    "        elif p<ints[5]:\n",
    "            return poss[5]\n",
    "        elif p<ints[6]:\n",
    "            return poss[6]\n",
    "        else:\n",
    "            return poss[7]       \n",
    "    \n",
    "test_data['POSITION'] = test_data['POSITION'].map(h)\n",
    "\n",
    "print(test_data.notna().count())\n",
    "\n",
    "\n",
    "test_data.to_csv('./metadataTestFilled.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.DataFrame(index=[1,2,3,4,5,6,7,8], data ={'name':['Melanoma','Melanocytic nevus',\n",
    "'Basal cell carcinoma',\n",
    " 'Actinic keratosis',\n",
    " 'Benign keratosis',\n",
    " 'Dermatofibroma', \n",
    " 'Vascular lesion',\n",
    " 'Squamous cell carcinoma']},)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading metadata\n",
    "data = pd.read_csv('metadataTrainFilled.csv')\n",
    "print('Number of training images:',data['ID'].count())\n",
    "print('Missing number of sexes in training set:',data['SEX'].isna().sum())\n",
    "print('Missing number of ages in training set:',data['AGE'].isna().sum())\n",
    "print('Missing number of locations in training set:',data['POSITION'].isna().sum())\n",
    "data.head()\n",
    "\n",
    "test = pd.read_csv('metadataTestFilled.csv')\n",
    "print('Number of testing images:',test['ID'].count())\n",
    "print('Missing number of sexes in testing set:',test['SEX'].isna().sum())\n",
    "print('Missing number of ages in testing set:',test['AGE'].isna().sum())\n",
    "print('Missing number of locations in testing set:',test['POSITION'].isna().sum())\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform exisiting metadata into numbers between 0 and 1\n",
    "def f(x):\n",
    "    if x == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "data['SEX'] = data['SEX'].map(f)\n",
    "test['SEX'] = test['SEX'].map(f)\n",
    "\n",
    "agenp = data['AGE'].to_numpy()\n",
    "#min_age = np.min(agenp) # is actually 0 so not I can just divide by the maximum\n",
    "max_age = np.max(agenp)\n",
    "\n",
    "def g(x):\n",
    "    return x/max_age\n",
    "    \n",
    "data['AGE'] = data['AGE'].map(g)\n",
    "test['AGE'] = test['AGE'].map(g)\n",
    "\n",
    "poss = np.unique(data['POSITION'].to_numpy())\n",
    "def h(x):\n",
    "    if x == poss[0]:\n",
    "        return 0\n",
    "    elif x == poss[2]:\n",
    "        return 0.14\n",
    "    elif x == poss[6]:\n",
    "        return 0.29\n",
    "    elif x == poss[1]:\n",
    "        return 0.43\n",
    "    elif x == poss[4]:\n",
    "        return 0.57\n",
    "    elif x == poss[7]:\n",
    "        return 0.71\n",
    "    elif x == poss[3]:\n",
    "        return 0.86\n",
    "    elif x == poss[5]:\n",
    "        return 1\n",
    "    \n",
    "data['POSITION'] = data['POSITION'].map(h)\n",
    "test['POSITION'] = test['POSITION'].map(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_features(mask):\n",
    "    #Area\n",
    "    area = np.sum(mask)\n",
    "\n",
    "    if area>2:\n",
    "        #Perimeter\n",
    "        cont_test = np.zeros(mask.shape)\n",
    "        Contour, Hierarchy = cv.findContours(mask.astype(np.uint8), cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\n",
    "        cv.drawContours(cont_test, Contour, len(Hierarchy[0])-1, 1, 1)\n",
    "        perim = np.sum(cont_test)\n",
    "        #Convex hull\n",
    "        coo = np.where(mask>0)\n",
    "        x = coo[0]\n",
    "        y = coo[1]\n",
    "        xm,ym = np.mean(x),np.mean(y)\n",
    "        coord = np.array([[a,b] for a,b in zip(x,y)])\n",
    "        try:\n",
    "            hull = ConvexHull(coord)\n",
    "            conv_perim = hull.area\n",
    "            conv_area = hull.volume\n",
    "            #Compactness, Roundness, Convexity, Solidity\n",
    "            compac = 4*np.pi*area/(perim**2)\n",
    "            roundn = 4*np.pi*area/(conv_perim**2)\n",
    "            convex = conv_perim/perim\n",
    "            solid = area/conv_area\n",
    "        except:\n",
    "            conv_perim = 0\n",
    "            conv_area = 0\n",
    "            compac = 0\n",
    "            roundn = 0\n",
    "            convex = 0\n",
    "            solid = 0\n",
    "        #Center of bb and aspect ratio\n",
    "        suml = np.sum(cont_test,axis=0)\n",
    "        sumc = np.sum(cont_test,axis=1)\n",
    "        indl = np.where(suml>0)[0]\n",
    "        indc = np.where(sumc>0)[0]\n",
    "        x1,x2,y1,y2 = indl[0],indl[-1],indc[0],indc[-1]\n",
    "        xc,yc = (x1+x2)/2,(y1+y2)/2\n",
    "        height,width = np.min((x2-x1,y2-y1)),np.max((x2-x1,y2-y1))\n",
    "        aspectratio = height/width\n",
    "        dist = np.sqrt((xc-xm)**2+(yc-ym)**2)\n",
    "    else:\n",
    "        perim = 0\n",
    "        aspectratio = 0\n",
    "        dist = 0\n",
    "        compac = 0\n",
    "        roundn = 0\n",
    "        convex = 0\n",
    "        solid = 0\n",
    "\n",
    "    return area,perim,aspectratio,dist,compac,roundn,convex,solid\n",
    "\n",
    "def channel_features(im,mask):\n",
    "\n",
    "    b,g,r = cv.split(im)\n",
    "    hls = cv.cvtColor(im,cv.COLOR_BGR2HLS)\n",
    "    h,l,_ = cv.split(hls)\n",
    "    #Min, Max, Average and Variance of L\n",
    "    mil = np.min(l)\n",
    "    mal = np.max(l)\n",
    "    avl = np.mean(l)\n",
    "    val = np.var(l)\n",
    "    #Min, Max, Average and Variance of H\n",
    "    mih = np.min(h)\n",
    "    mah = np.max(h)\n",
    "    avh = np.mean(h)\n",
    "    vah = np.var(h)\n",
    "    #Values of B,G,R,H and L inside the lesion and outside of it\n",
    "    bc = np.mean(b*mask)\n",
    "    be = np.mean(b*(1-mask))\n",
    "    gc = np.mean(g*mask)\n",
    "    ge = np.mean(g*(1-mask))\n",
    "    rc = np.mean(r*mask)\n",
    "    re = np.mean(r*(1-mask))\n",
    "    hc = np.mean(h*mask)\n",
    "    he = np.mean(h*(1-mask))\n",
    "    lc = np.mean(l*mask)\n",
    "    le = np.mean(l*(1-mask))\n",
    "    #Difference between the above values\n",
    "    diffb = abs(bc-be)\n",
    "    diffg = abs(gc-ge)\n",
    "    diffr = abs(rc-re)\n",
    "    diffh = abs(hc-he)\n",
    "    diffl = abs(lc-le)\n",
    "\n",
    "\n",
    "    return mil,mal,avl,val,mih,mah,avh,vah,diffb,diffg,diffr,diffh,diffl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute features for the train set and save them in a csv file\n",
    "names = data['ID'].to_numpy()\n",
    "minl,maxl,avgl,varl,minh,maxh,avgh,varh,diffb,diffg,diffr,diffh,diffl = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "area,perimeter,aspectratio,distance,compactness,roundness,convexity,solidity = [],[],[],[],[],[],[],[]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "\n",
    "    path = './Train/Train/'+names[i]+'.jpg'\n",
    "    im = cv.resize(cv.imread(path),(512,384))\n",
    "    maskpath = './Train/Modified/'+names[i]+'_seg.png'\n",
    "    mask = 1/255*cv.imread(maskpath,cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    mil,mal,avl,val,mih,mah,avh,vah,dib,dig,di_r,dih,dil = channel_features(im,mask)\n",
    "    ar,perim,asp,dist,compact,roundn,convex,solid = seg_features(mask)\n",
    "    \n",
    "    minl.append(mil)\n",
    "    maxl.append(mal)\n",
    "    avgl.append(avl)\n",
    "    varl.append(val)\n",
    "\n",
    "    minh.append(mih)\n",
    "    maxh.append(mah)\n",
    "    avgh.append(avh)\n",
    "    varh.append(vah)\n",
    "\n",
    "    diffb.append(dib)\n",
    "    diffg.append(dig)\n",
    "    diffr.append(di_r)\n",
    "    diffh.append(dih)\n",
    "    diffl.append(dil)\n",
    "\n",
    "    area.append(ar)\n",
    "    perimeter.append(perim)\n",
    "    aspectratio.append(asp)\n",
    "    distance.append(dist)\n",
    "    compactness.append(compact)\n",
    "    roundness.append(roundn)\n",
    "    convexity.append(convex)\n",
    "    solidity.append(solid)\n",
    "\n",
    "#Put values between 0 and 1\n",
    "minl = (minl-np.min(minl))/(np.max(minl)-np.min(minl))\n",
    "maxl = (maxl-np.min(maxl))/(np.max(maxl)-np.min(maxl))\n",
    "avgl = (avgl-np.min(avgl))/(np.max(avgl)-np.min(avgl))\n",
    "varl = (varl-np.min(varl))/(np.max(varl)-np.min(varl))\n",
    "\n",
    "minh = (minh-np.min(minh))/(np.max(minh)-np.min(minh))\n",
    "maxh = (maxh-np.min(maxh))/(np.max(maxh)-np.min(maxh))\n",
    "avgh = (avgh-np.min(avgh))/(np.max(avgh)-np.min(avgh))\n",
    "varh = (varh-np.min(varh))/(np.max(varh)-np.min(varh))\n",
    "\n",
    "diffb = (diffb-np.min(diffb))/(np.max(diffb)-np.min(diffb))\n",
    "diffg = (diffg-np.min(diffg))/(np.max(diffg)-np.min(diffg))\n",
    "diffr = (diffr-np.min(diffr))/(np.max(diffr)-np.min(diffr))\n",
    "diffh = (diffh-np.min(diffh))/(np.max(diffh)-np.min(diffh))\n",
    "diffl = (diffl-np.min(diffl))/(np.max(diffl)-np.min(diffl))\n",
    "\n",
    "area = (area-np.min(area))/(np.max(area)-np.min(area))\n",
    "perimeter = (perimeter-np.min(perimeter))/(np.max(perimeter)-np.min(perimeter))\n",
    "distance = (distance-np.min(distance))/(np.max(distance)-np.min(distance))\n",
    "\n",
    "\n",
    "id = data['ID'].to_numpy()\n",
    "clas = data['CLASS'].to_numpy()\n",
    "sex = data['SEX'].to_numpy()\n",
    "age = data['AGE'].to_numpy()\n",
    "pos = data['POSITION'].to_numpy()\n",
    "#Had to put a maximum of 1 for thee values that shouldn't exceed it by definition\n",
    "compactness = np.minimum(compactness,1)\n",
    "roundness = np.minimum(roundness,1)\n",
    "convexity = np.minimum(convexity,1.3)/1.3\n",
    "solidity = np.minimum(solidity,1)\n",
    "\n",
    "dic = {'ID':id,'CLASS':clas,'SEX':sex,'AGE':age,'POSITION':pos,'MinL':minl,'MaxL':maxl,'AvgL':avgl,'VarL':varl,'MinH':minh,'MaxH':maxh,'AvgH':avgh,'VarH':varh,'DiffB':diffb,\n",
    "        'DiffG':diffg,'DiffR':diffr,'DiffH':diffh,'DiffL':diffl,'Area':area,'Perimeter':perimeter,'Aspect Ratio':aspectratio,'Distance':distance,'Compactness':compactness,\n",
    "        'Roundness':roundness,'Convexity':convexity,'Solidity':solidity}\n",
    "print(len(dic.keys()))\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df = df.fillna(0)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv('./MetadataTrainModified.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the features for the test set\n",
    "names = test['ID'].to_numpy()\n",
    "minl,maxl,avgl,varl,minh,maxh,avgh,varh,diffb,diffg,diffr,diffh,diffl = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "area,perimeter,aspectratio,distance,compactness,roundness,convexity,solidity = [],[],[],[],[],[],[],[]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "\n",
    "    path = './Test/Test/'+names[i]+'.jpg'\n",
    "    im = cv.resize(cv.imread(path),(512,384))\n",
    "    maskpath = './Test/Modified/'+names[i]+'_seg.png'\n",
    "    mask = 1/255*cv.imread(maskpath,cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    mil,mal,avl,val,mih,mah,avh,vah,dib,dig,di_r,dih,dil = channel_features(im,mask)\n",
    "    ar,perim,asp,dist,compact,roundn,convex,solid = seg_features(mask)\n",
    "    \n",
    "    minl.append(mil)\n",
    "    maxl.append(mal)\n",
    "    avgl.append(avl)\n",
    "    varl.append(val)\n",
    "\n",
    "    minh.append(mih)\n",
    "    maxh.append(mah)\n",
    "    avgh.append(avh)\n",
    "    varh.append(vah)\n",
    "\n",
    "    diffb.append(dib)\n",
    "    diffg.append(dig)\n",
    "    diffr.append(di_r)\n",
    "    diffh.append(dih)\n",
    "    diffl.append(dil)\n",
    "\n",
    "    area.append(ar)\n",
    "    perimeter.append(perim)\n",
    "    aspectratio.append(asp)\n",
    "    distance.append(dist)\n",
    "    compactness.append(compact)\n",
    "    roundness.append(roundn)\n",
    "    convexity.append(convex)\n",
    "    solidity.append(solid)\n",
    "\n",
    "#Put values between 0 and 1\n",
    "minl = (minl-np.min(minl))/(np.max(minl)-np.min(minl))\n",
    "maxl = (maxl-np.min(maxl))/(np.max(maxl)-np.min(maxl))\n",
    "avgl = (avgl-np.min(avgl))/(np.max(avgl)-np.min(avgl))\n",
    "varl = (varl-np.min(varl))/(np.max(varl)-np.min(varl))\n",
    "\n",
    "minh = (minh-np.min(minh))/(np.max(minh)-np.min(minh))\n",
    "maxh = (maxh-np.min(maxh))/(np.max(maxh)-np.min(maxh))\n",
    "avgh = (avgh-np.min(avgh))/(np.max(avgh)-np.min(avgh))\n",
    "varh = (varh-np.min(varh))/(np.max(varh)-np.min(varh))\n",
    "\n",
    "diffb = (diffb-np.min(diffb))/(np.max(diffb)-np.min(diffb))\n",
    "diffg = (diffg-np.min(diffg))/(np.max(diffg)-np.min(diffg))\n",
    "diffr = (diffr-np.min(diffr))/(np.max(diffr)-np.min(diffr))\n",
    "diffh = (diffh-np.min(diffh))/(np.max(diffh)-np.min(diffh))\n",
    "diffl = (diffl-np.min(diffl))/(np.max(diffl)-np.min(diffl))\n",
    "\n",
    "area = (area-np.min(area))/(np.max(area)-np.min(area))\n",
    "perimeter = (perimeter-np.min(perimeter))/(np.max(perimeter)-np.min(perimeter))\n",
    "distance = (distance-np.min(distance))/(np.max(distance)-np.min(distance))\n",
    "\n",
    "id = test['ID'].to_numpy()\n",
    "sex = test['SEX'].to_numpy()\n",
    "age = test['AGE'].to_numpy()\n",
    "pos = test['POSITION'].to_numpy()\n",
    "#Had to put a maximum of 1 for thee values that shouldn't exceed it by definition\n",
    "compactness = np.minimum(compactness,1)\n",
    "roundness = np.minimum(roundness,1)\n",
    "convexity = np.minimum(convexity,1.3)/1.3\n",
    "solidity = np.minimum(solidity,1)\n",
    "\n",
    "dic = {'ID':id,'SEX':sex,'AGE':age,'POSITION':pos,'MinL':minl,'MaxL':maxl,'AvgL':avgl,'VarL':varl,'MinH':minh,'MaxH':maxh,'AvgH':avgh,'VarH':varh,'DiffB':diffb,\n",
    "        'DiffG':diffg,'DiffR':diffr,'DiffH':diffh,'DiffL':diffl,'Area':area,'Perimeter':perimeter,'Aspect Ratio':aspectratio,'Distance':distance,'Compactness':compactness,\n",
    "        'Roundness':roundness,'Convexity':convexity,'Solidity':solidity}\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.fillna(0)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_csv('./MetadataTestModified.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new metadata\n",
    "train = pd.read_csv('MetadataTrainModified.csv')\n",
    "print(train.head())\n",
    "test = pd.read_csv('MetadataTestModified.csv')\n",
    "print(test.head())\n",
    "\n",
    "#Convert to numpy array\n",
    "t = train.to_numpy()\n",
    "print(t.shape)\n",
    "t2 = test.to_numpy()\n",
    "print(t2.shape)\n",
    "\n",
    "#Separate between classes and metadata\n",
    "train_labels = np.array(t[:,1]).astype(int)\n",
    "#print(train_labels)\n",
    "train_data = t[:,2:]\n",
    "#print(train_data)\n",
    "test_data = t2[:,1:]\n",
    "#print(test_data)\n",
    "ids = t2[:,0]\n",
    "\n",
    "#Apply onehotencoder to improve results on certain classifiers\n",
    "train_lab = train_labels.reshape(-1,1)\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(train_lab)\n",
    "labels = ohe.transform(train_lab).toarray()\n",
    "#print(labels.shape)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(train_data,labels)\n",
    "print(reg.score(train_data,labels))\n",
    "\n",
    "test_labels = reg.predict(test_data)\n",
    "test_labels = np.argmax(test_labels,axis=1)+1\n",
    "print(test_labels)\n",
    "\n",
    "dict = {'ID':ids,'CLASS':test_labels}\n",
    "df.pd.DataFrame(dict)\n",
    "df.to_csv('./LinearClassif.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitcv= LogisticRegressionCV(Cs=[0.5,0.75,1],cv=10,penalty='l2',class_weight='balanced',solver='saga',max_iter=1000 ,multi_class='multinomial')\n",
    "logitcv.fit(train_data,train_labels)\n",
    "print(logitcv.score(train_data,train_labels))\n",
    "print(logitcv.C_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logit = LogisticRegression(penalty='l2',C=0.5,class_weight='balanced',solver='saga',max_iter=10000 ,multi_class='multinomial')\n",
    "logit.fit(train_data,train_labels)\n",
    "print(logit.score(train_data,train_labels))\n",
    "\n",
    "test_labels = logit.predict(test_data)\n",
    "print(test_labels)\n",
    "\n",
    "dict = {'ID':ids,'CLASS':test_labels}\n",
    "df.pd.DataFrame(dict)\n",
    "df.to_csv('./Logit.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy',random_state=23)\n",
    "tree.fit(train_data,labels)\n",
    "print(tree.score(train_data,labels))\n",
    "test_labels = tree.predict(test_data)\n",
    "test_labels = np.argmax(test_labels,axis=1)+1\n",
    "\n",
    "dict = {'ID':ids,'CLASS':test_labels}\n",
    "df.pd.DataFrame(dict)\n",
    "df.to_csv('./DecisionTree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF =RandomForestClassifier(criterion='entropy',random_state=23)\n",
    "p_grid_RF = {'n_estimators': [10,15,20,25,30], 'min_samples_leaf': [2,3,4,5,6], 'max_features': ['sqrt','log2']}   \n",
    "\n",
    "grid_RF = GridSearchCV(estimator=RF, param_grid=p_grid_RF, scoring='balanced_accuracy', cv=5,verbose=2)\n",
    "grid_RF.fit(train_data,labels)\n",
    "print(grid_RF.best_score_)\n",
    "print(grid_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = RandomForestClassifier(n_estimators=100,criterion='entropy',max_features='sqrt',random_state=23)\n",
    "trees.fit(train_data,labels)\n",
    "print(trees.score(train_data,labels))\n",
    "test_labels = trees.predict(test_data)\n",
    "test_labels = np.argmax(test_labels,axis=1)+1\n",
    "\n",
    "dict = {'ID':ids,'CLASS':test_labels}\n",
    "df.pd.DataFrame(dict)\n",
    "df.to_csv('./RandomForest.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grid={'C':[10,15,20],'gamma':[0.1,0.2,0.5,1]}\n",
    "grid_svm = GridSearchCV(SVC(),p_grid,scoring='balanced_accuracy',verbose=3)\n",
    "grid_svm.fit(train_data,train_labels)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(verbose=True,C=20,gamma=1,class_weight='balanced')\n",
    "svm.fit(train_data,train_labels)\n",
    "print(svm.score(train_data,train_labels))\n",
    "test_labels = svm.predict(test_data)\n",
    "\n",
    "dict = {'ID':ids,'CLASS':test_labels}\n",
    "df.pd.DataFrame(dict)\n",
    "df.to_csv('./SVM.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
